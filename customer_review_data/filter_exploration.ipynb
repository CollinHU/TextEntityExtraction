{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import wordpunct_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def stem(w):\n",
    "    return stemmer.stem(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_sentence(s):\n",
    "        s = re.sub(r' \\.', ' ',s)\n",
    "        s = re.sub(r'#+',' ',s)\n",
    "        s = re.sub(r'\\*+',' ',s)\n",
    "        s = re.sub(r'_+',' ',s)\n",
    "        s = re.sub(r':+',' ',s)\n",
    "        s = re.sub(r'\\(+',' ',s)\n",
    "        s = re.sub(r'\\)+',' ',s)\n",
    "        s = re.sub(r'\\|+',' ',s)\n",
    "        s = re.sub(r'\\\\\\w+',' ',s)\n",
    "        s = re.sub(r'=',' ',s)\n",
    "        s = re.sub(r'/+',' ',s)\n",
    "        s = re.sub(r'\\\\+',' ',s)\n",
    "        s = re.sub(r'[^\\x00-\\x7f]',' ', s)\n",
    "        #s = re.sub(r'[[:digit:]]',' ',s)\n",
    "        s = re.sub(r'\\s+',' ',s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opinion</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'excel': ['excellent', 'excellent'], 'easi': ...</td>\n",
       "      <td>{'pictur': ['picture', 'picture', 'picture', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'cool': ['cool'], 'first': ['first'], 'digit'...</td>\n",
       "      <td>{'toyyep': ['toyyep'], 'camera': ['camera', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'incred': ['incredible'], 'extens': ['extensi...</td>\n",
       "      <td>{'canon': ['canon', 'canon'], 'g3': ['g3', 'g3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'great': ['great'], 'satisfi': ['satisfied'],...</td>\n",
       "      <td>{'camera': ['camera', 'cameras', 'camera'], 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'full': ['full'], 'wonder': ['wonderful'], 'p...</td>\n",
       "      <td>{'practic': ['practice'], 'love': ['love'], 'c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             opinion  \\\n",
       "0  {'excel': ['excellent', 'excellent'], 'easi': ...   \n",
       "1  {'cool': ['cool'], 'first': ['first'], 'digit'...   \n",
       "2  {'incred': ['incredible'], 'extens': ['extensi...   \n",
       "3  {'great': ['great'], 'satisfi': ['satisfied'],...   \n",
       "4  {'full': ['full'], 'wonder': ['wonderful'], 'p...   \n",
       "\n",
       "                                              target  \n",
       "0  {'pictur': ['picture', 'picture', 'picture', '...  \n",
       "1  {'toyyep': ['toyyep'], 'camera': ['camera', 'c...  \n",
       "2  {'canon': ['canon', 'canon'], 'g3': ['g3', 'g3...  \n",
       "3  {'camera': ['camera', 'cameras', 'camera'], 'c...  \n",
       "4  {'practic': ['practice'], 'love': ['love'], 'c...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction = pd.read_csv('transactions.csv',index_col = 0)\n",
    "transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>excellent picture quality color i recently pur...</td>\n",
       "      <td>{'pictur qualiti color': ['picture quality col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cool toyyep this is my first digital camera , ...</td>\n",
       "      <td>{'digit camera': ['digital camera'], 'technic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>canon g3 ? incredible camera i did extensive r...</td>\n",
       "      <td>{'canon g3': ['canon g3'], 'incred camera': ['...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great camera i bought my canon g3 about a mont...</td>\n",
       "      <td>{'great camera': ['great camera'], 'canon g3':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>have n't had practice but i 'm already in love...</td>\n",
       "      <td>{'camera': ['camera'], 'full day': ['full day'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0  excellent picture quality color i recently pur...   \n",
       "1  cool toyyep this is my first digital camera , ...   \n",
       "2  canon g3 ? incredible camera i did extensive r...   \n",
       "3  great camera i bought my canon g3 about a mont...   \n",
       "4  have n't had practice but i 'm already in love...   \n",
       "\n",
       "                                              target  \n",
       "0  {'pictur qualiti color': ['picture quality col...  \n",
       "1  {'digit camera': ['digital camera'], 'technic ...  \n",
       "2  {'canon g3': ['canon g3'], 'incred camera': ['...  \n",
       "3  {'great camera': ['great camera'], 'canon g3':...  \n",
       "4  {'camera': ['camera'], 'full day': ['full day'...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transaction = pd.read_csv('Canon_G3_transaction_v3.csv',index_col = 0)\n",
    "new_transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targets = transaction['target'].values\n",
    "size = len(targets)\n",
    "G_dict = {}\n",
    "for i in range(size):\n",
    "    L_dict = eval(targets[i])\n",
    "    if len(targets[i]) == 1:\n",
    "        continue\n",
    "    for key in L_dict.keys():\n",
    "        G_dict[key] = G_dict.get(key, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "{'pictur': 26, 'qualiti': 24, 'color': 9, 'canon': 26, 'g3': 25, 'purchas': 4, 'camera': 44, 'fact': 5, 'trip': 4, 'week': 6, 'box': 5, 'way': 9, 'work': 6, 'g2': 10, 'set': 10, 'flash': 11, 'card': 10, 'line': 5, 'featur': 21, 'abil': 5, 'lens': 5, 'filter': 4, 'choic': 7, 'anyon': 6, 'use': 6, 'flexibl': 4, 'option': 9, 'softwar': 9, 'detail': 4, 'everyth': 7, 'month': 7, 'megapixel': 11, 'manual': 5, 'control': 10, 'function': 4, 'len': 19, 'imag': 14, 'cf': 5, 'type': 5, 'thing': 10, 'noth': 4, 'viewfind': 12, 'lcd': 13, 'research': 6, 'review': 11, 'resolut': 7, 'speed': 5, 'mode': 13, 'photo': 17, 'shutter': 7, 'flaw': 7, 'time': 18, 'photographi': 5, 'day': 5, 'peopl': 4, 'screen': 7, 'adjust': 4, 't': 6, 'focus': 5, 'slr': 5, 'exposur': 4, 'auto': 8, 'rang': 7, 'market': 5, 'photograph': 6, 'zoom': 11, 'result': 5, 'film': 6, 'button': 6, 'batteri': 14, 'problem': 7, 'view': 4, 'price': 8, 'point': 13, 'shoot': 9, 'year': 5, 'pic': 4, 'store': 5, 'hand': 7, 'bit': 6, 'life': 9, 'power': 4, 'shot': 7, 'someth': 6, 'lot': 6, 'nois': 4, 'experi': 4, 'light': 5, 'printer': 4, 'nikon': 5, 'technolog': 4, 'anyth': 4, 'photoshop': 4, 'format': 4}\n"
     ]
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for key in G_dict.keys():\n",
    "    if G_dict[key] > 3:\n",
    "        new_dict[key] = G_dict[key]\n",
    "#print(new_dict)\n",
    "print(len(new_dict))\n",
    "print(new_dict)\n",
    "f2 = open(\"Canon_G3_extracted_target.txt\",'w')\n",
    "for target in new_dict:\n",
    "    f2.write(target+'\\n')\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "477\n",
      "125\n",
      "{'pictur qualiti color': 2, 'purchas': 4, 'camera': 40, 'fact': 5, 'pictur': 19, 'box': 4, 'way': 9, 'g2': 10, 'canon': 14, 'pictur qualiti': 4, 'featur': 17, 'abil': 4, 'extern flash': 3, 'anyon': 4, 'use': 3, 'flexibl': 2, 'digit camera': 21, 'everyth': 7, 'month': 6, 'g3': 20, 'powershot g3': 3, 'megapixel': 5, 'function': 3, 'len': 10, 'flash': 4, 'good choic': 2, 'imag': 5, 'thing': 5, 'noth': 4, 'lcd': 7, 'canon g3': 8, 'extens research': 2, 'megapixel camera': 3, 'research': 4, 'detail': 3, 'review': 3, 'automat set': 2, 'mode': 3, 'great camera': 6, 'photo': 8, 'qualiti': 8, 'viewfind': 10, 'time': 15, 'design flaw': 2, 'digit photographi': 2, 'photo qualiti': 3, 'peopl': 2, 't': 3, 'focus': 3, 'slr': 2, 'option': 4, 'lcd screen': 4, 'result': 5, 'week': 4, 'film': 4, 'button': 5, 'optic zoom': 4, 'digit pictur': 2, 'view': 3, 'price': 3, 'point': 7, 'shoot': 4, 'year': 5, 'work': 5, 'great pictur': 3, 'control': 6, 'batteri': 3, 'softwar': 4, 'pic': 4, 'auto mode': 4, 'store': 3, 'hand': 7, 'bit': 5, 'batteri life': 7, 'other camera': 4, 'mb card': 2, 'photograph': 2, 'same time': 3, 'color': 5, 'long time': 2, 'shutter speed': 2, 'manual': 3, 'manual mode': 3, 'someth': 5, 'photographi': 2, 'simpl point': 2, 'day': 2, 'best camera': 2, 'market': 3, 'digit imag': 2, '4x zoom': 2, 'price rang': 2, 'experi': 2, 'shot': 4, 'lot': 5, 'nois': 2, 'light': 2, 'high qualiti': 2, 'built-in flash': 3, 'technolog': 3, 'anyth': 4, 'shutter': 5, 'nikon slr': 2, 'canon camera': 2, 'exposur': 2, 'rang': 2, 'photoshop': 3, 'posit review': 2, 'differ camera': 2, 'set': 2, 'other review': 2, 'imag qualiti': 3, 'nikon': 2, 'problem': 3, 'low light': 2, '5mp camera': 2, 'compactflash card': 2, 'film camera': 2, '4mp camera': 2, 'trip': 2, 'backup batteri': 2, 'flaw': 2, 'new technolog': 2, 'lens': 2, 'compact flash': 2}\n"
     ]
    }
   ],
   "source": [
    "new_transaction = pd.read_csv('Canon_G3_transaction_v3.csv',index_col = 0)\n",
    "new_transaction.head()\n",
    "targets = new_transaction['target'].values\n",
    "size = len(targets)\n",
    "print(size)\n",
    "G_dict = {}\n",
    "for i in range(size):\n",
    "    L_dict = eval(targets[i])\n",
    "    if len(targets[i]) == 1:\n",
    "        continue\n",
    "    for key in L_dict.keys():\n",
    "        G_dict[key] = G_dict.get(key, 0) + 1\n",
    "print(len(G_dict))\n",
    "new_G_dict = {}\n",
    "for key in G_dict.keys():\n",
    "    if G_dict[key]  >= 2:\n",
    "        new_G_dict[key] = G_dict[key]\n",
    "print(len(new_G_dict))\n",
    "print(new_G_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458\n",
      "977\n",
      "334\n",
      "['purchas', 'camera', 'fact', 'pictur', 'press', 'box', 'way', 'g2', 'canon', 'x', 'featur', 'abil', 'anyon', 'combin', 'eas', 'use', 'flexibl', 'toy', 'everyth', 'month', 'g3', 'megapixel', 'novic', 'function', 'len', 'flash', 'minut', 'imag', 'thing', 'noth', 'lcd', 'coupl', 'model', 'research', 'detail', 'dpreview', 'review', 'speed', 'mode', 'hundr', 'photo', 'qualiti', 'viewfind', 'time', 'love', 'fun', 'peopl', 'screen', 'top', 'reason', 't', 'focus', 'slr', 'program', 'option', 'meter', '4mp', 'result', 'week', 'film', 'system', 'button', 'negat', 'direct', 'sort', 'upload', 'comput', 'view', 'menus', 'price', 'point', 'shoot', 'year', 'work', 'period', 'control', 'batteri', 'softwar', 'pic', 'store', 'hand', 'bit', 'type', 'photograph', 'wife', 'print', 'color', 'output', 'amazon', 'ton', 'kid', 'manual', 'dial', 'apertur', 'weight', 'someth', 'photographi', 'auto', 'day', 'market', 'sharp', 'b', 'issu', 'lever', 'lack', 'experi', 'shot', 'life', 'action', 'grain', 'lot', 'nois', 'light', 'object', 'printer', 'cost', 'limit', 'digicam', 'reput', 'technolog', 'anyth', 'lock', 'child', 'shutter', 'subject', 'money', 'metz', 'ct-4', 'sca', 'order', 'sync', 'durat', 'exposur', 'spot', 'rang', 'wall', 'file', 'hardwar', 'photoshop', 'decis', 'drawback', 'mp', 'servic', 'competit', 'posit', 'set', 'effect', 'devic', 'sun', 'user', 'term', 'nikon', 'slrs', 'problem', 'cp5000', 'feel', 'buyer', 'pros', 'none', 'delay', 'one', 'bug', 'moment', 'attach', 'red-ey', 'friend', 'process', 'iphoto', 'equip', 'eo', 'cap', 'case', 'brand', 'trip', 'note', 'need', 'art', 'home', 'onlin', 'shortcom', 'lag', 'import', 'curv', 'flaw', 'product', 'situat', 'lens', 'expens', 'plastic', 'captur', 'depth', 'shoe', 'strap', 'part', 'access', 'opinion', 'right', 'resolut', 'resol', 'stori', 'distort', 'compactflash', 'subnotebook', 'cf', 'differ', 'card', 'complaint', 'finder', 's330', 'head', 'tripod', 'level', 'disappoint', 'portrait', 'extens', 'enthusiast', 'choic', 'optic', 'storag', 'visabl loss', 'cool toyyep', 'singl cent', 'littl overview', 'powershot seri', 'great number', 'display panel', 'accur adjust', 'diopter adjust', 'same size', 'elph line', '14x zoom', 'nice addit', 'digit zoom', 'best bargain', 'advanc photobug', 'imagin newbi', 'pure class', 'singl charg', 'great colorimetri', 'other compani', 'more power', 'knockoff charger', 'juli fire', 'quick spin', 'focal length', 'averag joe', 'new g', 'great deal', 'white balanc', '4x zoom', 'minim effort', 'first move', 'frame sensor', 'connector cord', 'densiti filter', 'attend shadow', 'plug-in show', 'k.', 'best buy', 'consum magazin', 'perfect condit', 'redwood forest', 'other flower', 'legitim criticsm', 'telephoto doubler', 'great fan', 'great gain', 'raw format', 'better balanc', 'f5 .6', 'nd filter', 'white edg', 'white offset', 'less haze', 'tonal balanc', 'big mistak', 'few second', 'olympus c5050', 'format olympus c5050', 'metal bodi', 'mani adjust', 'snap dim', 'rebel ti', 'fli adjust', 'bodi construct', 'perfect perform', 'big plus', 'hard drive', 'more info', 'champagn finish', 'silver magnesium finish', 'excel grip', 'digit elph', 'good compromis', 'soni dcs-f', 'round deal', 'tiff format', 'nice compromis', 'beauti design', 'damag idea', 'most purpos', 'general rule', 'dark place', 'greatest expect', 'factor buff', 'small handbag', 'electron item', 'person usag', 'parallax phenomenon', 'full charg', 'usb connect', 'drive space', 'onli suggest', 'clean wipe', 'mm adapt', 'left corner', 'playback zoom', 'expedit ship', 'longer zoom', 'more adjust', 'remot unit', 'annoy shadow', 'real powerhous', 'birthday parti', 'aperatur prioriti', 'steel heft', 'gb drive', 'intern memori', 'bottom line', 'digit point-and-shoot', 'custom white-bal', 'white piec', 'soni dsc-f', 'minor nit']\n"
     ]
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for key in G_dict.keys():\n",
    "    tk_key = nltk.word_tokenize(key)\n",
    "    if len(tk_key) == 1:\n",
    "        new_dict[tk_key[0]] = tk_key\n",
    "for key in G_dict.keys():\n",
    "    i = len(new_dict)\n",
    "    tk_key = nltk.word_tokenize(key)\n",
    "    flag = False\n",
    "    if len(tk_key) > 1:\n",
    "        for tk_k in tk_key:\n",
    "            if tk_k in new_dict.keys():\n",
    "                new_dict[tk_k].append(key) \n",
    "                flag =True\n",
    "        if flag == False:\n",
    "            new_dict[key] = [key]\n",
    "print(len(new_dict))\n",
    "print(len(G_dict))\n",
    "#print(new_dict)\n",
    "count_dict = {}\n",
    "for key in new_dict.keys():\n",
    "    count_dict[key] = 0\n",
    "    for item in new_dict[key]:\n",
    "        count_dict[key] += G_dict[item]\n",
    "count_dict\n",
    "new_count_dict = {}\n",
    "for key in count_dict.keys():\n",
    "    if len(word_tokenize(key)) >= 2:\n",
    "        new_count_dict[key] = count_dict[key]\n",
    "        continue\n",
    "    if count_dict[key] >= 2:\n",
    "        new_count_dict[key] = count_dict[key]\n",
    "print(len(new_count_dict))\n",
    "print(list(new_count_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = \"Canon_G3.csv\"\n",
    "opinion_file = \"opinion_list.txt\"\n",
    "target_file = \"{}_target.txt\".format(data_file.split('.')[0])\n",
    "f2 = open(target_file,'r')\n",
    "lines = f2.readlines()\n",
    "target_list = [line.split(\"\\n\")[0] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for key in target_list:\n",
    "    tk_key = nltk.word_tokenize(key)\n",
    "    if len(tk_key) == 1:\n",
    "        new_dict[tk_key[0]] = tk_key\n",
    "for key in target_list:\n",
    "    i = len(new_dict)\n",
    "    tk_key = nltk.word_tokenize(key)\n",
    "    flag = False\n",
    "    if len(tk_key) > 1:\n",
    "        for tk_k in tk_key:\n",
    "            if tk_k in new_dict.keys():\n",
    "                new_dict[tk_k].append(key) \n",
    "                flag =True\n",
    "        if flag == False:\n",
    "            new_dict[key] = [key]\n",
    "print(len(new_dict))\n",
    "target_list = list(new_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "target_dict = {}\n",
    "for key in target_list:\n",
    "    tk_key = nltk.word_tokenize(key)\n",
    "    if len(tk_key) == 1:\n",
    "        target_dict[tk_key[0]] = tk_key\n",
    "for key in target_list:\n",
    "    i = len(new_dict)\n",
    "    tk_key = nltk.word_tokenize(key)\n",
    "    flag = False\n",
    "    if len(tk_key) > 1:\n",
    "        for tk_k in tk_key:\n",
    "            if tk_k in target_dict.keys():\n",
    "                target_dict[tk_k].append(key)\n",
    "            else:\n",
    "                \n",
    "print(len(target_dict))\n",
    "target_list = list(target_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "camera\n",
      "pictur\n",
      "canon\n",
      "featur\n",
      "use\n",
      "g3\n",
      "function\n",
      "len\n",
      "flash\n",
      "imag\n",
      "lcd\n",
      "speed\n",
      "photo\n",
      "qualiti\n",
      "viewfind\n",
      "focus\n",
      "option\n",
      "4mp\n",
      "price\n",
      "control\n",
      "batteri\n",
      "softwar\n",
      "print\n",
      "color\n",
      "manual\n",
      "dial\n",
      "weight\n",
      "lever\n",
      "shot\n",
      "grain\n",
      "nois\n",
      "servic\n",
      "feel\n",
      "delay\n",
      "case\n",
      "lag\n",
      "import\n",
      "product\n",
      "depth\n",
      "strap\n",
      "distort\n",
      "compactflash\n",
      "optic\n",
      "raw format\n",
      "white offset\n",
      "tiff format\n",
      "46\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "P = len(target_list)\n",
    "print(P)\n",
    "#print(target_list)\n",
    "extracted_target_list = list(new_count_dict.keys())\n",
    "#print(extracted_target_list)\n",
    "count = 0\n",
    "for w in extracted_target_list:\n",
    "    if w in target_list:\n",
    "        print(w)\n",
    "        count += 1\n",
    "TP = set(target_list).intersection(extracted_target_list)\n",
    "print(len(TP))\n",
    "#print(TP)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "['design', 'compactflash', 'download', 'weight', 'nois', '4mp', 'product', 'unrespons', 'shot', 'optic', 'color', 'lag', 'manual', 'use', 'automod', 'remot', 'bodi', 'delay', 'flash', 'distort', 'option', 'finish', 'viewfind', 'servic', 'zoom', 'canon', 'import', 'highlight', 'photo', 'speed', 'function', 'dial', 'look', 'case', 'pictur', 'batteri', 'macro', 'camera', 'size', 'print', 'lever', 'len', 'qualiti', 'perform', 'display', 'menu', 'feel', 'softwar', 'g3', 'shape', 'canera', 'price', 'featur', 'control', 'strap', 'depth', 'imag', 'grain', 'focus', 'made', 'lcd', 'learn', 'white offset', 'no-off button', 'raw format', 'night mode', 'tiff format', 'auto mode', 'memori card', 'auto set', 'light auto correct', 'four megapixel', 'spot meter']\n",
      "25\n",
      "{'featur', 'use', 'viewfind', 'batteri', 'nois', 'len', 'control', 'imag', 'manual', 'auto mode', 'qualiti', 'pictur', 'photo', 'camera', 'softwar', 'canon', 'flash', 'lcd', 'function', 'color', 'option', 'shot', 'g3', 'price', 'focus'}\n"
     ]
    }
   ],
   "source": [
    "P = len(target_list)\n",
    "print(P)\n",
    "print(target_list)\n",
    "extracted_target_list = list(new_G_dict.keys())\n",
    "#print(extracted_target_list)\n",
    "TP = set(target_list).intersection(extracted_target_list)\n",
    "print(len(TP))\n",
    "print(TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### new_dict = {}\n",
    "for key in G_dict.keys():\n",
    "    if G_dict[key] >1:\n",
    "        new_dict[key] = G_dict[key]\n",
    "print(len(new_dict))\n",
    "print(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = new_transaction['comment'][2]\n",
    "s =process_sentence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'cours': ['courses', 'course'], 'code exampl': ['code examples'], 'algorithm applic': ['algorithm applications'], 'intuit': ['intuition'], 'schedul': ['schedules'], 'instructor': ['instructors'], 'capston project': ['capstone project']}\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transaction['target'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'emili fox': ['Emily Fox'], 'job': ['job'], 'concept': ['concepts'], 'explan': ['explanation'], 'compon': ['components'], 'formula': ['formulas'], 'materi': ['materials'], 'cours': ['course'], 'univers': ['University'], 'machin learn special': ['machine learning specialization']}\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transaction['target'][99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"Emily Fox did a great job in explaining tough concepts with simple explanation of the components in the formulas It's a little tough to get through the materials though, it's the 4th course in University of Washington's machine learning specialization afterall =\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from nltk.parse.corenlp import CoreNLPServer, CoreNLPDependencyParser\\npath_to_jar = '/Users/collin/stanford/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar'\\npath_to_models_jar = '/Users/collin/stanford/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0-models.jar'\\nserver = CoreNLPServer(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\\nserver.start()\\ndependency_parser = CoreNLPDependencyParser()\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from nltk.parse.corenlp import CoreNLPServer, CoreNLPDependencyParser\n",
    "path_to_jar = '/Users/collin/stanford/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar'\n",
    "path_to_models_jar = '/Users/collin/stanford/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0-models.jar'\n",
    "server = CoreNLPServer(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "server.start()\n",
    "dependency_parser = CoreNLPDependencyParser()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependency_parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ddb0e9dabb35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdep_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dependency_parser' is not defined"
     ]
    }
   ],
   "source": [
    "result = dependency_parser.raw_parse(s)\n",
    "dep = next(result)\n",
    "dep_list = dict(sorted(dep.nodes.items()))\n",
    "print(dep_list)\n",
    "for _, node in sorted(dep.nodes.items()):\n",
    "    print(node)\n",
    "    oo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_update(target_dict, new_w, word):\n",
    "    if new_w in target_dict.keys():\n",
    "        target_dict[new_w].append(word)\n",
    "    else:\n",
    "        target_dict[new_w] = [word]\n",
    "def extract_target_phase(target_dict, s):\n",
    "    ##parse the sentence\n",
    "    result = dependency_parser.raw_parse(s)\n",
    "    dep = next(result)\n",
    "    dep_list = dict(sorted(dep.nodes.items()))\n",
    "\n",
    "    for _, node in sorted(dep.nodes.items()):\n",
    "         if node['word'] is not None:\n",
    "            w = stem(node['word'])\n",
    "            dep_dict = dict(node['deps'])\n",
    "            if w in target_list and node['tag'][:2] == 'NN' and 'compound' in dep_dict.keys():\n",
    "                #print(w)\n",
    "                new_w =''\n",
    "                word = ''\n",
    "                for index in dep_dict['compound']:\n",
    "                    comp_word = dep_list[index]['word']\n",
    "                    comp_w = stem(comp_word)\n",
    "                    new_w = new_w + ' ' + comp_w\n",
    "                    word = word + ' ' + comp_word\n",
    "                new_w = new_w + ' ' + w\n",
    "                word = word + ' ' + node['word']\n",
    "                dict_update(target_dict, new_w, word)\n",
    "            elif w in target_list and node['tag'][:2] == 'NN' and node['rel'] !='compound':\n",
    "                dict_update(target_dict, w, node['word'])\n",
    "\n",
    "def update_target(sents):\n",
    "    target_d = {}\n",
    "    sent_list = sent_tokenize(sents)\n",
    "    for sent in sent_list:\n",
    "        extract_target_phase(target_d,sent)\n",
    "    for key in target_d.keys():\n",
    "        target_d[key] = list(set(target_d[key]))\n",
    "    return target_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependency_parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a048dff016e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtarget_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-451296aab057>\u001b[0m in \u001b[0;36mupdate_target\u001b[0;34m(sents)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0msent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mextract_target_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtarget_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-451296aab057>\u001b[0m in \u001b[0;36mextract_target_phase\u001b[0;34m(target_dict, s)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_target_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m##parse the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdep_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dependency_parser' is not defined"
     ]
    }
   ],
   "source": [
    "f1 = open('opinion_list_1.txt','r')\n",
    "lines = f1.readlines()\n",
    "opinion_list =[line.split(\"\\n\")[0] for line in lines]\n",
    "\n",
    "f2 = open('target_list_1.txt','r')\n",
    "lines = f2.readlines()\n",
    "target_list = [line.split(\"\\n\")[0] for line in lines]\n",
    "update_target(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "server.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
