{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import wordpunct_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def stem(w):\n",
    "    return stemmer.stem(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_sentence(s):\n",
    "        s = re.sub(r' \\.', ' ',s)\n",
    "        s = re.sub(r'#+',' ',s)\n",
    "        s = re.sub(r'\\*+',' ',s)\n",
    "        s = re.sub(r'_+',' ',s)\n",
    "        s = re.sub(r':+',' ',s)\n",
    "        s = re.sub(r'\\(+',' ',s)\n",
    "        s = re.sub(r'\\)+',' ',s)\n",
    "        s = re.sub(r'\\|+',' ',s)\n",
    "        s = re.sub(r'\\\\\\w+',' ',s)\n",
    "        s = re.sub(r'=',' ',s)\n",
    "        s = re.sub(r'/+',' ',s)\n",
    "        s = re.sub(r'\\\\+',' ',s)\n",
    "        s = re.sub(r'[^\\x00-\\x7f]',' ', s)\n",
    "        #s = re.sub(r'[[:digit:]]',' ',s)\n",
    "        s = re.sub(r'\\s+',' ',s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opinion</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'good': ['Good']}</td>\n",
       "      <td>{'cours': ['course'], 'lot': ['lot'], 'thank':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'great': ['great'], 'good': ['good'], 'plan':...</td>\n",
       "      <td>{'cours': ['course', 'courses'], 'code': ['cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'nice': ['Nice', 'nice'], 'practic': ['practi...</td>\n",
       "      <td>{'cours': ['course'], 'stuff': ['stuffs'], 'an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'tougher': ['tougher'], 'complex': ['complex']}</td>\n",
       "      <td>{'cluster': ['Clustering'], 'retriev': ['Retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{}</td>\n",
       "      <td>{'instructor': ['instructor'], 'differ': ['dif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             opinion  \\\n",
       "0                                 {'good': ['Good']}   \n",
       "1  {'great': ['great'], 'good': ['good'], 'plan':...   \n",
       "2  {'nice': ['Nice', 'nice'], 'practic': ['practi...   \n",
       "3   {'tougher': ['tougher'], 'complex': ['complex']}   \n",
       "4                                                 {}   \n",
       "\n",
       "                                              target  \n",
       "0  {'cours': ['course'], 'lot': ['lot'], 'thank':...  \n",
       "1  {'cours': ['course', 'courses'], 'code': ['cod...  \n",
       "2  {'cours': ['course'], 'stuff': ['stuffs'], 'an...  \n",
       "3  {'cluster': ['Clustering'], 'retriev': ['Retri...  \n",
       "4  {'instructor': ['instructor'], 'differ': ['dif...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction = pd.read_csv('transactions.csv',index_col = 0)\n",
    "transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good course. Learned a lot from it. Thanks</td>\n",
       "      <td>{'cours': ['course'], 'lot': ['lot'], 'thank':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This course was great With good code examples ...</td>\n",
       "      <td>{'cours': ['courses', 'course'], 'code exampl'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nice course with all the practical stuffs and ...</td>\n",
       "      <td>{'cours': ['course'], 'stuff': ['stuffs'], 'an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clustering &amp;amp; Retrieval was a lot tougher c...</td>\n",
       "      <td>{'cluster': ['Clustering'], 'retriev': ['Retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm a grad student and I can notice the instru...</td>\n",
       "      <td>{'instructor': ['instructor'], 'differ': ['dif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0        Good course. Learned a lot from it. Thanks    \n",
       "1  This course was great With good code examples ...   \n",
       "2  Nice course with all the practical stuffs and ...   \n",
       "3  Clustering &amp; Retrieval was a lot tougher c...   \n",
       "4  I'm a grad student and I can notice the instru...   \n",
       "\n",
       "                                              target  \n",
       "0  {'cours': ['course'], 'lot': ['lot'], 'thank':...  \n",
       "1  {'cours': ['courses', 'course'], 'code exampl'...  \n",
       "2  {'cours': ['course'], 'stuff': ['stuffs'], 'an...  \n",
       "3  {'cluster': ['Clustering'], 'retriev': ['Retri...  \n",
       "4  {'instructor': ['instructor'], 'differ': ['dif...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transaction = pd.read_csv('transaction_v3.csv',index_col = 0)\n",
    "new_transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = transaction['target'].values\n",
    "size = len(targets)\n",
    "G_dict = {}\n",
    "for i in range(size):\n",
    "    L_dict = eval(targets[i])\n",
    "    for key in L_dict.keys():\n",
    "        G_dict[key] = G_dict.get(key, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accomplish': 1,\n",
       " 'algorithm': 6,\n",
       " 'analysi': 1,\n",
       " 'analyt': 1,\n",
       " 'applic': 1,\n",
       " 'approach': 1,\n",
       " 'aspect': 1,\n",
       " 'asses': 1,\n",
       " 'assign': 6,\n",
       " 'awesom': 1,\n",
       " 'background': 4,\n",
       " 'beginn': 1,\n",
       " 'bit': 3,\n",
       " 'book': 2,\n",
       " 'capston': 6,\n",
       " 'carlo': 4,\n",
       " 'challeng': 1,\n",
       " 'cliff': 1,\n",
       " 'cluster': 10,\n",
       " 'code': 4,\n",
       " 'compon': 1,\n",
       " 'concept': 8,\n",
       " 'content': 4,\n",
       " 'cours': 69,\n",
       " 'coverag': 1,\n",
       " 'date': 1,\n",
       " 'dendrogram': 1,\n",
       " 'depth': 3,\n",
       " 'detail': 3,\n",
       " 'differ': 1,\n",
       " 'dive': 1,\n",
       " 'emili': 6,\n",
       " 'encount': 1,\n",
       " 'end': 1,\n",
       " 'etho': 1,\n",
       " 'everyth': 1,\n",
       " 'exampl': 1,\n",
       " 'experi': 1,\n",
       " 'explan': 4,\n",
       " 'feedback': 2,\n",
       " 'formula': 1,\n",
       " 'forum': 1,\n",
       " 'foundat': 3,\n",
       " 'fox': 2,\n",
       " 'good': 1,\n",
       " 'grip': 1,\n",
       " 'hit': 1,\n",
       " 'homework': 1,\n",
       " 'idea': 2,\n",
       " 'import': 1,\n",
       " 'insight': 3,\n",
       " 'instructor': 6,\n",
       " 'introduct': 1,\n",
       " 'intuit': 2,\n",
       " 'job': 2,\n",
       " 'knn': 1,\n",
       " 'knowledg': 4,\n",
       " 'lab': 2,\n",
       " 'languag': 1,\n",
       " 'lda': 5,\n",
       " 'learn': 10,\n",
       " 'learner': 2,\n",
       " 'list': 1,\n",
       " 'lot': 9,\n",
       " 'machin': 9,\n",
       " 'manner': 1,\n",
       " 'materi': 12,\n",
       " 'math': 3,\n",
       " 'method': 2,\n",
       " 'middl': 1,\n",
       " 'ml': 8,\n",
       " 'mooc': 1,\n",
       " 'other': 1,\n",
       " 'pace': 2,\n",
       " 'panda': 1,\n",
       " 'part': 3,\n",
       " 'peer': 1,\n",
       " 'peopl': 3,\n",
       " 'point': 1,\n",
       " 'practic': 1,\n",
       " 'project': 3,\n",
       " 'qualiti': 2,\n",
       " 'read': 2,\n",
       " 'reduct': 1,\n",
       " 'research': 1,\n",
       " 'respect': 1,\n",
       " 'retriev': 4,\n",
       " 'retriv': 1,\n",
       " 'review': 1,\n",
       " 'schedul': 1,\n",
       " 'section': 1,\n",
       " 'sens': 1,\n",
       " 'someon': 1,\n",
       " 'special': 10,\n",
       " 'specif': 1,\n",
       " 'stuff': 2,\n",
       " 'subject': 2,\n",
       " 'survey': 1,\n",
       " 'system': 1,\n",
       " 'task': 2,\n",
       " 'teacher': 2,\n",
       " 'techniqu': 2,\n",
       " 'term': 1,\n",
       " 'thank': 5,\n",
       " 'theori': 3,\n",
       " 'time': 4,\n",
       " 'tool': 2,\n",
       " 'topic': 8,\n",
       " 'treatment': 1,\n",
       " 'tree': 1,\n",
       " 'understand': 5,\n",
       " 'use': 1,\n",
       " 'user': 1,\n",
       " 'valu': 1,\n",
       " 'video': 1,\n",
       " 'view': 1,\n",
       " 'way': 2,\n",
       " 'web': 1,\n",
       " 'week': 3}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cours': 65, 'lot': 9, 'thank': 4, 'intuit': 2, 'instructor': 6, ' capston project': 3, 'stuff': 2, 'topic': 7, 'part': 3, 'lda': 4, 'cluster': 8, 'retriev': 4, 'emili': 4, 'knowledg': 4, 'peopl': 3, 'teacher': 2, 'book': 2, 'concept': 6, 'qualiti': 2, 'depth': 2, 'special': 8, ' machin learn': 5, 'understand': 5, ' classif cours': 2, 'math': 2, 'learn': 3, 'background': 2, 'tool': 2, 'ml': 3, 'assign': 5, 'materi': 12, 'way': 2, 'time': 4, ' math background': 2, 'week': 3, 'method': 2, 'feedback': 2, 'learner': 2, ' advanc topic': 2, 'pace': 2, 'code': 3, 'carlo': 3, 'insight': 3, 'capston': 2, 'job': 2, 'bit': 3, 'theori': 3, 'detail': 3, 'idea': 2, 'content': 4, 'algorithm': 3, 'foundat': 2, 'explan': 3, 'subject': 2}\n"
     ]
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for key in G_dict.keys():\n",
    "    if G_dict[key] >= 2:\n",
    "        new_dict[key] = G_dict[key]\n",
    "print(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = new_transaction['target'].values\n",
    "size = len(targets)\n",
    "G_dict = {}\n",
    "for i in range(size):\n",
    "    L_dict = eval(targets[i])\n",
    "    for key in L_dict.keys():\n",
    "        G_dict[key] = G_dict.get(key, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accomplish': 1,\n",
       " 'advanc topic': 2,\n",
       " 'algorithm': 3,\n",
       " 'algorithm applic': 1,\n",
       " 'analysi': 1,\n",
       " 'andrew ng cours': 1,\n",
       " 'approach': 1,\n",
       " 'aspect': 1,\n",
       " 'asses': 1,\n",
       " 'assign': 5,\n",
       " 'awesom': 1,\n",
       " 'background': 2,\n",
       " 'beginn': 1,\n",
       " 'bit': 3,\n",
       " 'book': 2,\n",
       " 'capston': 2,\n",
       " 'capston project': 3,\n",
       " 'carlo': 3,\n",
       " 'carlo excel cours': 1,\n",
       " 'challeng': 1,\n",
       " 'classif cours': 2,\n",
       " 'cliff': 1,\n",
       " 'cluster': 8,\n",
       " 'cluster cours': 1,\n",
       " 'cluster task': 1,\n",
       " 'code': 3,\n",
       " 'code exampl': 1,\n",
       " 'compon': 1,\n",
       " 'concept': 6,\n",
       " 'content': 4,\n",
       " 'cours': 65,\n",
       " 'coverag': 1,\n",
       " 'date': 1,\n",
       " 'deep learn capston': 1,\n",
       " 'dendrogram': 1,\n",
       " 'depth': 2,\n",
       " 'depth explan': 1,\n",
       " 'detail': 3,\n",
       " 'differ': 1,\n",
       " 'dimension reduct': 1,\n",
       " 'dive': 1,\n",
       " 'em algorithm': 1,\n",
       " 'emili': 4,\n",
       " 'emili fox': 1,\n",
       " 'encount': 1,\n",
       " 'etho': 1,\n",
       " 'everyth': 1,\n",
       " 'experi': 1,\n",
       " 'expert emili': 1,\n",
       " 'explan': 3,\n",
       " 'feedback': 2,\n",
       " 'formula': 1,\n",
       " 'forum': 1,\n",
       " 'foundat': 2,\n",
       " 'foundat cours': 1,\n",
       " 'good': 1,\n",
       " 'graph lab': 1,\n",
       " 'graphlab user': 1,\n",
       " 'grip': 1,\n",
       " 'hand-on lab machin': 1,\n",
       " 'hit': 1,\n",
       " 'homework': 1,\n",
       " 'idea': 2,\n",
       " 'import': 1,\n",
       " 'insight': 3,\n",
       " 'instructor': 6,\n",
       " 'introduct': 1,\n",
       " 'intuit': 2,\n",
       " 'job': 2,\n",
       " 'kd tree': 1,\n",
       " 'knn': 1,\n",
       " 'knowledg': 4,\n",
       " 'languag': 1,\n",
       " 'lda': 4,\n",
       " 'learn': 3,\n",
       " 'learn task': 1,\n",
       " 'learner': 2,\n",
       " 'list': 1,\n",
       " 'lot': 9,\n",
       " 'machin': 1,\n",
       " 'machin learn': 5,\n",
       " 'machin learn special': 1,\n",
       " 'machin learn techniqu': 1,\n",
       " 'manner': 1,\n",
       " 'match concept': 1,\n",
       " 'materi': 12,\n",
       " 'math': 2,\n",
       " 'math background': 2,\n",
       " 'method': 2,\n",
       " 'middl': 1,\n",
       " 'ml': 3,\n",
       " 'ml algorithm': 1,\n",
       " 'ml concept': 1,\n",
       " 'ml research': 1,\n",
       " 'ml special': 1,\n",
       " 'mooc': 1,\n",
       " 'other': 1,\n",
       " 'pace': 2,\n",
       " 'panda': 1,\n",
       " 'part': 3,\n",
       " 'peer': 1,\n",
       " 'peopl': 3,\n",
       " 'point': 1,\n",
       " 'practic': 1,\n",
       " 'prof fox': 1,\n",
       " 'professor end': 1,\n",
       " 'program assign': 1,\n",
       " 'qualiti': 2,\n",
       " 'read': 1,\n",
       " 'read materi': 1,\n",
       " 'recommend system': 1,\n",
       " 'respect': 1,\n",
       " 'retriev': 4,\n",
       " 'retriv': 1,\n",
       " 'review': 1,\n",
       " 'schedul': 1,\n",
       " 'section': 1,\n",
       " 'sens': 1,\n",
       " 'someon': 1,\n",
       " 'special': 8,\n",
       " 'specif': 1,\n",
       " 'stuff': 2,\n",
       " 'subject': 2,\n",
       " 'survey': 1,\n",
       " 'teacher': 2,\n",
       " 'techniqu': 1,\n",
       " 'term': 1,\n",
       " 'thank': 4,\n",
       " 'theori': 3,\n",
       " 'time': 4,\n",
       " 'tool': 2,\n",
       " 'topic': 7,\n",
       " 'treatment': 1,\n",
       " 'understand': 5,\n",
       " 'use': 1,\n",
       " 'valu': 1,\n",
       " 'video': 1,\n",
       " 'view': 1,\n",
       " 'way': 2,\n",
       " 'web': 1,\n",
       " 'week': 3,\n",
       " 'world analyt': 1}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cours': 65, 'lot': 9, 'thank': 4, 'intuit': 2, 'instructor': 6, ' capston project': 3, 'stuff': 2, 'topic': 7, 'part': 3, 'lda': 4, 'cluster': 8, 'retriev': 4, 'emili': 4, 'knowledg': 4, 'peopl': 3, 'teacher': 2, 'book': 2, 'concept': 6, 'qualiti': 2, 'depth': 2, 'special': 8, ' machin learn': 5, 'understand': 5, ' classif cours': 2, 'math': 2, 'learn': 3, 'background': 2, 'tool': 2, 'ml': 3, 'assign': 5, 'materi': 12, 'way': 2, 'time': 4, ' math background': 2, 'week': 3, 'method': 2, 'feedback': 2, 'learner': 2, ' advanc topic': 2, 'pace': 2, 'code': 3, 'carlo': 3, 'insight': 3, 'capston': 2, 'job': 2, 'bit': 3, 'theori': 3, 'detail': 3, 'idea': 2, 'content': 4, 'algorithm': 3, 'foundat': 2, 'explan': 3, 'subject': 2}\n"
     ]
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for key in G_dict.keys():\n",
    "    if G_dict[key] >= 2:\n",
    "        new_dict[key] = G_dict[key]\n",
    "print(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = new_transaction['comment'][99]\n",
    "s =process_sentence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'emili': ['Emily'], 'fox': ['Fox'], 'job': ['job'], 'concept': ['concepts'], 'explan': ['explanation'], 'compon': ['components'], 'formula': ['formulas'], 'materi': ['materials'], 'cours': ['course'], 'machin': ['machine'], 'learn': ['learning'], 'special': ['specialization']}\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction['target'][99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{' emili fox': [' Emily Fox'], 'job': ['job'], 'concept': ['concepts'], 'explan': ['explanation'], 'compon': ['components'], 'formula': ['formulas'], 'materi': ['materials'], 'cours': ['course'], ' machin learn special': [' machine learning specialization']}\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transaction['target'][99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"Emily Fox did a great job in explaining tough concepts with simple explanation of the components in the formulas It's a little tough to get through the materials though, it's the 4th course in University of Washington's machine learning specialization afterall =\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPServer, CoreNLPDependencyParser\n",
    "path_to_jar = '/Users/collin/stanford/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar'\n",
    "path_to_models_jar = '/Users/collin/stanford/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0-models.jar'\n",
    "server = CoreNLPServer(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "server.start()\n",
    "dependency_parser = CoreNLPDependencyParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependency_parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-ddb0e9dabb35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdep_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dependency_parser' is not defined"
     ]
    }
   ],
   "source": [
    "result = dependency_parser.raw_parse(s)\n",
    "dep = next(result)\n",
    "dep_list = dict(sorted(dep.nodes.items()))\n",
    "print(dep_list)\n",
    "for _, node in sorted(dep.nodes.items()):\n",
    "    print(node)\n",
    "    oo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_update(target_dict, new_w, word):\n",
    "    if new_w in target_dict.keys():\n",
    "        target_dict[new_w].append(word)\n",
    "    else:\n",
    "        target_dict[new_w] = [word]\n",
    "def extract_target_phase(target_dict, s):\n",
    "    ##parse the sentence\n",
    "    result = dependency_parser.raw_parse(s)\n",
    "    dep = next(result)\n",
    "    dep_list = dict(sorted(dep.nodes.items()))\n",
    "\n",
    "    for _, node in sorted(dep.nodes.items()):\n",
    "         if node['word'] is not None:\n",
    "            w = stem(node['word'])\n",
    "            dep_dict = dict(node['deps'])\n",
    "            if w in target_list and node['tag'][:2] == 'NN' and 'compound' in dep_dict.keys():\n",
    "                #print(w)\n",
    "                new_w =''\n",
    "                word = ''\n",
    "                for index in dep_dict['compound']:\n",
    "                    comp_word = dep_list[index]['word']\n",
    "                    comp_w = stem(comp_word)\n",
    "                    new_w = new_w + ' ' + comp_w\n",
    "                    word = word + ' ' + comp_word\n",
    "                new_w = new_w + ' ' + w\n",
    "                word = word + ' ' + node['word']\n",
    "                dict_update(target_dict, new_w, word)\n",
    "            elif w in target_list and node['tag'][:2] == 'NN' and node['rel'] !='compound':\n",
    "                dict_update(target_dict, w, node['word'])\n",
    "\n",
    "def update_target(sents):\n",
    "    target_d = {}\n",
    "    sent_list = sent_tokenize(sents)\n",
    "    for sent in sent_list:\n",
    "        extract_target_phase(target_d,sent)\n",
    "    for key in target_d.keys():\n",
    "        target_d[key] = list(set(target_d[key]))\n",
    "    return target_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' emili fox': [' Emily Fox'],\n",
       " ' machin learn special': [' machine learning specialization'],\n",
       " 'compon': ['components'],\n",
       " 'concept': ['concepts'],\n",
       " 'cours': ['course'],\n",
       " 'explan': ['explanation'],\n",
       " 'formula': ['formulas'],\n",
       " 'job': ['job'],\n",
       " 'materi': ['materials']}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = open('opinion_list_1.txt','r')\n",
    "lines = f1.readlines()\n",
    "opinion_list =[line.split(\"\\n\")[0] for line in lines]\n",
    "\n",
    "f2 = open('target_list_1.txt','r')\n",
    "lines = f2.readlines()\n",
    "target_list = [line.split(\"\\n\")[0] for line in lines]\n",
    "update_target(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "server.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
