{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import wordpunct_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opinion</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'good': ['Good']}</td>\n",
       "      <td>{'cours': ['course'], 'lot': ['lot'], 'thank':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'great': ['great'], 'good': ['good'], 'plan':...</td>\n",
       "      <td>{'cours': ['course', 'courses'], 'code': ['cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'nice': ['Nice', 'nice'], 'practic': ['practi...</td>\n",
       "      <td>{'cours': ['course'], 'stuff': ['stuffs'], 'an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'tougher': ['tougher'], 'complex': ['complex']}</td>\n",
       "      <td>{'cluster': ['Clustering'], 'retriev': ['Retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{}</td>\n",
       "      <td>{'instructor': ['instructor'], 'differ': ['dif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             opinion  \\\n",
       "0                                 {'good': ['Good']}   \n",
       "1  {'great': ['great'], 'good': ['good'], 'plan':...   \n",
       "2  {'nice': ['Nice', 'nice'], 'practic': ['practi...   \n",
       "3   {'tougher': ['tougher'], 'complex': ['complex']}   \n",
       "4                                                 {}   \n",
       "\n",
       "                                              target  \n",
       "0  {'cours': ['course'], 'lot': ['lot'], 'thank':...  \n",
       "1  {'cours': ['course', 'courses'], 'code': ['cod...  \n",
       "2  {'cours': ['course'], 'stuff': ['stuffs'], 'an...  \n",
       "3  {'cluster': ['Clustering'], 'retriev': ['Retri...  \n",
       "4  {'instructor': ['instructor'], 'differ': ['dif...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction = pd.read_csv('transactions.csv',index_col = 0)\n",
    "transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good course. Learned a lot from it. Thanks</td>\n",
       "      <td>{'cours': ['course'], 'lot': ['lot'], 'thank':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This course was great With good code examples ...</td>\n",
       "      <td>{'cours': ['course', 'courses'], 'code exampl'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nice course with all the practical stuffs and ...</td>\n",
       "      <td>{'cours': ['course'], 'stuff': ['stuffs'], 'an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clustering &amp;amp; Retrieval was a lot tougher c...</td>\n",
       "      <td>{'cluster': ['Clustering'], 'retriev': ['Retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm a grad student and I can notice the instru...</td>\n",
       "      <td>{'instructor': ['instructor'], 'differ': ['dif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0        Good course. Learned a lot from it. Thanks    \n",
       "1  This course was great With good code examples ...   \n",
       "2  Nice course with all the practical stuffs and ...   \n",
       "3  Clustering &amp; Retrieval was a lot tougher c...   \n",
       "4  I'm a grad student and I can notice the instru...   \n",
       "\n",
       "                                              target  \n",
       "0  {'cours': ['course'], 'lot': ['lot'], 'thank':...  \n",
       "1  {'cours': ['course', 'courses'], 'code exampl'...  \n",
       "2  {'cours': ['course'], 'stuff': ['stuffs'], 'an...  \n",
       "3  {'cluster': ['Clustering'], 'retriev': ['Retri...  \n",
       "4  {'instructor': ['instructor'], 'differ': ['dif...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transaction = pd.read_csv('transaction_v3.csv',index_col = 0)\n",
    "new_transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = transaction['target'].values\n",
    "size = len(targets)\n",
    "G_dict = {}\n",
    "for i in range(size):\n",
    "    L_dict = eval(targets[i])\n",
    "    for key in L_dict.keys():\n",
    "        G_dict[key] = G_dict.get(key, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "print(len(G_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cours': 69, 'lot': 9, 'thank': 5, 'code': 4, 'algorithm': 6, 'intuit': 2, 'instructor': 6, 'capston': 6, 'project': 3, 'stuff': 2, 'topic': 8, 'part': 3, 'lda': 5, 'cluster': 10, 'retriev': 4, 'concept': 8, 'emili': 6, 'knowledg': 4, 'peopl': 3, 'teacher': 2, 'book': 2, 'qualiti': 2, 'depth': 3, 'special': 10, 'machin': 9, 'learn': 10, 'understand': 5, 'ml': 8, 'math': 3, 'background': 4, 'tool': 2, 'lab': 2, 'foundat': 3, 'assign': 6, 'materi': 12, 'way': 2, 'instruct': 2, 'time': 4, 'week': 3, 'method': 2, 'feedback': 2, 'learner': 2, 'pace': 2, 'carlo': 4, 'insight': 3, 'fox': 2, 'job': 2, 'bit': 3, 'theori': 3, 'techniqu': 2, 'detail': 3, 'idea': 2, 'content': 4, 'explan': 4, 'read': 2, 'task': 2, 'subject': 2}\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for key in G_dict.keys():\n",
    "    if G_dict[key] >= 2:\n",
    "        new_dict[key] = G_dict[key]\n",
    "print(new_dict)\n",
    "print(len(new_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "{'cours': 65, 'lot': 9, 'thank': 4, 'intuit': 2, 'instructor': 6, 'capston project': 3, 'stuff': 2, 'topic': 7, 'part': 3, 'lda': 4, 'cluster': 8, 'retriev': 4, 'emili': 4, 'knowledg': 4, 'peopl': 3, 'teacher': 2, 'book': 2, 'concept': 6, 'qualiti': 2, 'depth': 2, 'special': 8, 'machin learn': 5, 'understand': 5, 'classif cours': 2, 'math': 2, 'learn': 3, 'background': 2, 'tool': 2, 'ml': 3, 'assign': 5, 'difficulti': 2, 'reason': 3, 'materi': 12, 'way': 2, 'instruct': 2, 'time': 4, 'math background': 2, 'week': 3, 'method': 2, 'feedback': 2, 'learner': 2, 'advanc topic': 2, 'pace': 2, 'code': 3, 'carlo': 3, 'insight': 3, 'capston': 2, 'job': 2, 'bit': 3, 'theori': 3, 'detail': 3, 'idea': 2, 'content': 4, 'algorithm': 3, 'foundat': 2, 'explan': 3, 'subject': 2}\n"
     ]
    }
   ],
   "source": [
    "targets = new_transaction['target'].values\n",
    "size = len(targets)\n",
    "G_dict = {}\n",
    "for i in range(size):\n",
    "    L_dict = eval(targets[i])\n",
    "    for key in L_dict.keys():\n",
    "        G_dict[key] = G_dict.get(key, 0) + 1\n",
    "new_G_dict = {}\n",
    "for key in G_dict.keys():\n",
    "    if G_dict[key] > 1:\n",
    "        new_G_dict[key] = G_dict[key]\n",
    "print(len(new_G_dict))\n",
    "print(new_G_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matching(key_one, key_two):\n",
    "    key_one_list = word_tokenize(key_one)\n",
    "    key_two_list = word_tokenize(key_two)\n",
    "    if key_two_list[:2] == key_one_list or key_two_list[-2:] == key_one_list:\n",
    "        return True\n",
    "    else:\n",
    "        return False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = 'a b'\n",
    "B = 'c a b'\n",
    "matching(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n",
      "175\n",
      "90\n",
      "{'cours': 65, 'lot': 9, 'thank': 4, 'code exampl': 1, 'algorithm applic': 1, 'intuit': 2, 'instructor': 6, 'capston project': 3, 'stuff': 2, 'topic': 7, 'part': 3, 'lda': 4, 'graphlab user': 1, 'cluster': 8, 'retriev': 4, 'match concept': 1, 'emili': 4, 'knowledg': 4, 'peopl': 3, 'teacher': 2, 'book': 2, 'concept': 6, 'qualiti': 2, 'depth': 2, 'special': 8, 'machin learn': 5, 'understand': 5, 'ml algorithm': 1, 'classif cours': 2, 'math': 2, 'learn': 3, 'recommend system': 1, 'background': 2, 'tool': 2, 'graph lab': 1, 'ml': 3, 'kd tree': 1, 'foundat cours': 1, 'assign': 5, 'graphlab technolog': 1, 'difficulti': 2, 'reason': 3, 'ml framework': 1, 'materi': 12, 'way': 2, 'instruct': 2, 'cluster cours': 1, 'time': 4, 'math background': 2, 'week': 3, 'gibb sampl': 1, 'method': 2, 'feedback': 2, 'learner': 2, 'advanc topic': 2, 'pace': 2, 'professor end': 1, 'expert emili': 1, 'code': 3, 'carlo': 3, 'insight': 3, 'dimension reduct': 1, 'capston': 2, 'prof fox': 1, 'job': 2, 'ml concept': 1, 'bit': 3, 'theori': 3, 'detail': 3, 'idea': 2, 'content': 4, 'depth explan': 1, 'world analyt': 1, 'algorithm': 3, 'foundat': 2, 'explan': 3, 'learn task': 1, 'cluster task': 1, 'subject': 2, 'ml research': 1, 'em algorithm': 1, 'read materi': 1, 'program assign': 1, 'ml special': 1, 'emili fox': 1, 'deep learn capston': 1, 'andrew ng cours': 1, 'python program skill': 1, 'hand-on lab machin': 1, 'carlo excel cours': 1}\n"
     ]
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for key in G_dict.keys():\n",
    "    tk_key = nltk.word_tokenize(key)\n",
    "    if len(tk_key) < 3:\n",
    "        new_dict[key] = G_dict[key]\n",
    "\n",
    "for key in G_dict.keys():\n",
    "    tk_key = nltk.word_tokenize(key)\n",
    "    flag = False\n",
    "    if len(tk_key) > 2:\n",
    "        for new_key in new_dict.keys():\n",
    "            if len(word_tokenize(new_key)) == 2:\n",
    "                flag = matching(new_key, key)\n",
    "            if flag:\n",
    "                new_dict[new_key] += G_dict[key]\n",
    "                break      \n",
    "        if flag == False:\n",
    "            new_dict[key] = G_dict[key]\n",
    "print(len(new_dict))\n",
    "print(len(G_dict))\n",
    "filter_dict = {}\n",
    "for key in new_dict.keys():\n",
    "    if len(word_tokenize(key)) > 1:\n",
    "        filter_dict[key] = G_dict[key]\n",
    "    elif new_dict[key] > 1:\n",
    "        filter_dict[key] = new_dict[key]\n",
    "print(len(filter_dict))\n",
    "print(filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cours': 65, 'lot': 9, 'thank': 4, 'intuit': 2, 'instructor': 6, 'capston project': 3, 'stuff': 2, 'topic': 7, 'part': 3, 'lda': 4, 'cluster': 8, 'retriev': 4, 'emili': 4, 'knowledg': 4, 'peopl': 3, 'teacher': 2, 'book': 2, 'concept': 6, 'qualiti': 2, 'depth': 2, 'special': 8, 'machin learn': 5, 'understand': 5, 'classif cours': 2, 'math': 2, 'learn': 3, 'background': 2, 'tool': 2, 'ml': 3, 'assign': 5, 'materi': 12, 'way': 2, 'time': 4, 'math background': 2, 'week': 3, 'method': 2, 'feedback': 2, 'learner': 2, 'advanc topic': 2, 'pace': 2, 'code': 3, 'carlo': 3, 'insight': 3, 'capston': 2, 'job': 2, 'bit': 3, 'theori': 3, 'detail': 3, 'idea': 2, 'content': 4, 'algorithm': 3, 'foundat': 2, 'explan': 3, 'subject': 2}\n"
     ]
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for key in G_dict.keys():\n",
    "    if G_dict[key] >= 2:\n",
    "        new_dict[key] = G_dict[key]\n",
    "print(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = new_transaction['comment'][2]\n",
    "s =process_sentence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'cours': ['courses', 'course'], 'code exampl': ['code examples'], 'algorithm applic': ['algorithm applications'], 'intuit': ['intuition'], 'schedul': ['schedules'], 'instructor': ['instructors'], 'capston project': ['capstone project']}\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transaction['target'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'emili fox': ['Emily Fox'], 'job': ['job'], 'concept': ['concepts'], 'explan': ['explanation'], 'compon': ['components'], 'formula': ['formulas'], 'materi': ['materials'], 'cours': ['course'], 'univers': ['University'], 'machin learn special': ['machine learning specialization']}\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transaction['target'][99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"Emily Fox did a great job in explaining tough concepts with simple explanation of the components in the formulas It's a little tough to get through the materials though, it's the 4th course in University of Washington's machine learning specialization afterall =\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from nltk.parse.corenlp import CoreNLPServer, CoreNLPDependencyParser\\npath_to_jar = '/Users/collin/stanford/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar'\\npath_to_models_jar = '/Users/collin/stanford/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0-models.jar'\\nserver = CoreNLPServer(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\\nserver.start()\\ndependency_parser = CoreNLPDependencyParser()\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from nltk.parse.corenlp import CoreNLPServer, CoreNLPDependencyParser\n",
    "path_to_jar = '/Users/collin/stanford/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar'\n",
    "path_to_models_jar = '/Users/collin/stanford/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0-models.jar'\n",
    "server = CoreNLPServer(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "server.start()\n",
    "dependency_parser = CoreNLPDependencyParser()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependency_parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ddb0e9dabb35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdep_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dependency_parser' is not defined"
     ]
    }
   ],
   "source": [
    "result = dependency_parser.raw_parse(s)\n",
    "dep = next(result)\n",
    "dep_list = dict(sorted(dep.nodes.items()))\n",
    "print(dep_list)\n",
    "for _, node in sorted(dep.nodes.items()):\n",
    "    print(node)\n",
    "    oo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependency_parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a048dff016e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtarget_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-451296aab057>\u001b[0m in \u001b[0;36mupdate_target\u001b[0;34m(sents)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0msent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mextract_target_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtarget_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-451296aab057>\u001b[0m in \u001b[0;36mextract_target_phase\u001b[0;34m(target_dict, s)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_target_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m##parse the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdep_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dependency_parser' is not defined"
     ]
    }
   ],
   "source": [
    "f1 = open('opinion_list_1.txt','r')\n",
    "lines = f1.readlines()\n",
    "opinion_list =[line.split(\"\\n\")[0] for line in lines]\n",
    "\n",
    "f2 = open('target_list_1.txt','r')\n",
    "lines = f2.readlines()\n",
    "target_list = [line.split(\"\\n\")[0] for line in lines]\n",
    "update_target(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'65'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"654321\"\n",
    "s[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
