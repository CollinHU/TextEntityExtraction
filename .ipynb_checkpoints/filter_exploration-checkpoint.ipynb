{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import wordpunct_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def stem(w):\n",
    "    return stemmer.stem(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_sentence(s):\n",
    "        s = re.sub(r' \\.', ' ',s)\n",
    "        s = re.sub(r'#+',' ',s)\n",
    "        s = re.sub(r'\\*+',' ',s)\n",
    "        s = re.sub(r'_+',' ',s)\n",
    "        s = re.sub(r':+',' ',s)\n",
    "        s = re.sub(r'\\(+',' ',s)\n",
    "        s = re.sub(r'\\)+',' ',s)\n",
    "        s = re.sub(r'\\|+',' ',s)\n",
    "        s = re.sub(r'\\\\\\w+',' ',s)\n",
    "        s = re.sub(r'=',' ',s)\n",
    "        s = re.sub(r'/+',' ',s)\n",
    "        s = re.sub(r'\\\\+',' ',s)\n",
    "        s = re.sub(r'[^\\x00-\\x7f]',' ', s)\n",
    "        #s = re.sub(r'[[:digit:]]',' ',s)\n",
    "        s = re.sub(r'\\s+',' ',s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opinion</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'good': ['Good']}</td>\n",
       "      <td>{'cours': ['course'], 'lot': ['lot'], 'thank':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'great': ['great'], 'good': ['good'], 'plan':...</td>\n",
       "      <td>{'cours': ['course', 'courses'], 'code': ['cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'nice': ['Nice', 'nice'], 'practic': ['practi...</td>\n",
       "      <td>{'cours': ['course'], 'stuff': ['stuffs'], 'an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'tougher': ['tougher'], 'complex': ['complex']}</td>\n",
       "      <td>{'cluster': ['Clustering'], 'retriev': ['Retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{}</td>\n",
       "      <td>{'instructor': ['instructor'], 'differ': ['dif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             opinion  \\\n",
       "0                                 {'good': ['Good']}   \n",
       "1  {'great': ['great'], 'good': ['good'], 'plan':...   \n",
       "2  {'nice': ['Nice', 'nice'], 'practic': ['practi...   \n",
       "3   {'tougher': ['tougher'], 'complex': ['complex']}   \n",
       "4                                                 {}   \n",
       "\n",
       "                                              target  \n",
       "0  {'cours': ['course'], 'lot': ['lot'], 'thank':...  \n",
       "1  {'cours': ['course', 'courses'], 'code': ['cod...  \n",
       "2  {'cours': ['course'], 'stuff': ['stuffs'], 'an...  \n",
       "3  {'cluster': ['Clustering'], 'retriev': ['Retri...  \n",
       "4  {'instructor': ['instructor'], 'differ': ['dif...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction = pd.read_csv('transactions.csv',index_col = 0)\n",
    "transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good course. Learned a lot from it. Thanks</td>\n",
       "      <td>{'cours': ['course'], 'lot': ['lot'], 'thank':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This course was great With good code examples ...</td>\n",
       "      <td>{'cours': ['courses', 'course'], 'code exampl'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nice course with all the practical stuffs and ...</td>\n",
       "      <td>{'cours': ['course'], 'stuff': ['stuffs'], 'an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clustering &amp;amp; Retrieval was a lot tougher c...</td>\n",
       "      <td>{'cluster': ['Clustering'], 'retriev': ['Retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm a grad student and I can notice the instru...</td>\n",
       "      <td>{'instructor': ['instructor'], 'differ': ['dif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0        Good course. Learned a lot from it. Thanks    \n",
       "1  This course was great With good code examples ...   \n",
       "2  Nice course with all the practical stuffs and ...   \n",
       "3  Clustering &amp; Retrieval was a lot tougher c...   \n",
       "4  I'm a grad student and I can notice the instru...   \n",
       "\n",
       "                                              target  \n",
       "0  {'cours': ['course'], 'lot': ['lot'], 'thank':...  \n",
       "1  {'cours': ['courses', 'course'], 'code exampl'...  \n",
       "2  {'cours': ['course'], 'stuff': ['stuffs'], 'an...  \n",
       "3  {'cluster': ['Clustering'], 'retriev': ['Retri...  \n",
       "4  {'instructor': ['instructor'], 'differ': ['dif...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transaction = pd.read_csv('transaction_v3.csv',index_col = 0)\n",
    "new_transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = transaction['target'].values\n",
    "size = len(targets)\n",
    "G_dict = {}\n",
    "for i in range(size):\n",
    "    L_dict = eval(targets[i])\n",
    "    for key in L_dict.keys():\n",
    "        G_dict[key] = G_dict.get(key, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accomplish': 1,\n",
       " 'algorithm': 6,\n",
       " 'analysi': 1,\n",
       " 'analyt': 1,\n",
       " 'applic': 1,\n",
       " 'approach': 1,\n",
       " 'aspect': 1,\n",
       " 'asses': 1,\n",
       " 'assign': 6,\n",
       " 'awesom': 1,\n",
       " 'background': 4,\n",
       " 'beginn': 1,\n",
       " 'bit': 3,\n",
       " 'book': 2,\n",
       " 'capston': 6,\n",
       " 'carlo': 4,\n",
       " 'challeng': 1,\n",
       " 'cliff': 1,\n",
       " 'cluster': 10,\n",
       " 'code': 4,\n",
       " 'compon': 1,\n",
       " 'concept': 8,\n",
       " 'content': 4,\n",
       " 'cours': 69,\n",
       " 'coverag': 1,\n",
       " 'date': 1,\n",
       " 'dendrogram': 1,\n",
       " 'depth': 3,\n",
       " 'detail': 3,\n",
       " 'differ': 1,\n",
       " 'dive': 1,\n",
       " 'emili': 6,\n",
       " 'encount': 1,\n",
       " 'end': 1,\n",
       " 'etho': 1,\n",
       " 'everyth': 1,\n",
       " 'exampl': 1,\n",
       " 'experi': 1,\n",
       " 'explan': 4,\n",
       " 'feedback': 2,\n",
       " 'formula': 1,\n",
       " 'forum': 1,\n",
       " 'foundat': 3,\n",
       " 'fox': 2,\n",
       " 'good': 1,\n",
       " 'grip': 1,\n",
       " 'hit': 1,\n",
       " 'homework': 1,\n",
       " 'idea': 2,\n",
       " 'import': 1,\n",
       " 'insight': 3,\n",
       " 'instructor': 6,\n",
       " 'introduct': 1,\n",
       " 'intuit': 2,\n",
       " 'job': 2,\n",
       " 'knn': 1,\n",
       " 'knowledg': 4,\n",
       " 'lab': 2,\n",
       " 'languag': 1,\n",
       " 'lda': 5,\n",
       " 'learn': 10,\n",
       " 'learner': 2,\n",
       " 'list': 1,\n",
       " 'lot': 9,\n",
       " 'machin': 9,\n",
       " 'manner': 1,\n",
       " 'materi': 12,\n",
       " 'math': 3,\n",
       " 'method': 2,\n",
       " 'middl': 1,\n",
       " 'ml': 8,\n",
       " 'mooc': 1,\n",
       " 'other': 1,\n",
       " 'pace': 2,\n",
       " 'panda': 1,\n",
       " 'part': 3,\n",
       " 'peer': 1,\n",
       " 'peopl': 3,\n",
       " 'point': 1,\n",
       " 'practic': 1,\n",
       " 'project': 3,\n",
       " 'qualiti': 2,\n",
       " 'read': 2,\n",
       " 'reduct': 1,\n",
       " 'research': 1,\n",
       " 'respect': 1,\n",
       " 'retriev': 4,\n",
       " 'retriv': 1,\n",
       " 'review': 1,\n",
       " 'schedul': 1,\n",
       " 'section': 1,\n",
       " 'sens': 1,\n",
       " 'someon': 1,\n",
       " 'special': 10,\n",
       " 'specif': 1,\n",
       " 'stuff': 2,\n",
       " 'subject': 2,\n",
       " 'survey': 1,\n",
       " 'system': 1,\n",
       " 'task': 2,\n",
       " 'teacher': 2,\n",
       " 'techniqu': 2,\n",
       " 'term': 1,\n",
       " 'thank': 5,\n",
       " 'theori': 3,\n",
       " 'time': 4,\n",
       " 'tool': 2,\n",
       " 'topic': 8,\n",
       " 'treatment': 1,\n",
       " 'tree': 1,\n",
       " 'understand': 5,\n",
       " 'univers': 1,\n",
       " 'use': 1,\n",
       " 'user': 1,\n",
       " 'valu': 1,\n",
       " 'video': 1,\n",
       " 'view': 1,\n",
       " 'way': 2,\n",
       " 'web': 1,\n",
       " 'week': 3}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cours': 69, 'lot': 9, 'thank': 5, 'code': 4, 'algorithm': 6, 'intuit': 2, 'instructor': 6, 'capston': 6, 'project': 3, 'stuff': 2, 'topic': 8, 'part': 3, 'lda': 5, 'cluster': 10, 'retriev': 4, 'concept': 8, 'emili': 6, 'knowledg': 4, 'peopl': 3, 'teacher': 2, 'book': 2, 'qualiti': 2, 'depth': 3, 'special': 10, 'machin': 9, 'learn': 10, 'understand': 5, 'ml': 8, 'math': 3, 'background': 4, 'tool': 2, 'lab': 2, 'foundat': 3, 'assign': 6, 'materi': 12, 'way': 2, 'time': 4, 'week': 3, 'method': 2, 'feedback': 2, 'learner': 2, 'pace': 2, 'carlo': 4, 'insight': 3, 'fox': 2, 'job': 2, 'bit': 3, 'theori': 3, 'techniqu': 2, 'detail': 3, 'idea': 2, 'content': 4, 'explan': 4, 'read': 2, 'task': 2, 'subject': 2}\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for key in G_dict.keys():\n",
    "    if G_dict[key] >= 2:\n",
    "        new_dict[key] = G_dict[key]\n",
    "print(new_dict)\n",
    "print(len(new_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = new_transaction['target'].values\n",
    "size = len(targets)\n",
    "G_dict = {}\n",
    "for i in range(size):\n",
    "    L_dict = eval(targets[i])\n",
    "    for key in L_dict.keys():\n",
    "        G_dict[key] = G_dict.get(key, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "144\n",
      "{'cours': ['cours', 'classif cours', 'foundat cours', 'andrew ng cours', 'cluster cours', 'carlo excel cours'], 'lot': ['lot'], 'thank': ['thank'], 'intuit': ['intuit'], 'schedul': ['schedul'], 'instructor': ['instructor'], 'stuff': ['stuff'], 'analysi': ['analysi'], 'topic': ['topic', 'advanc topic'], 'part': ['part'], 'lda': ['lda'], 'everyth': ['everyth'], 'cluster': ['cluster', 'cluster cours', 'cluster task'], 'retriev': ['retriev'], 'emili': ['emili', 'expert emili', 'emili fox'], 'differ': ['differ'], 'import': ['import'], 'knowledg': ['knowledg'], 'peopl': ['peopl'], 'teacher': ['teacher'], 'book': ['book'], 'concept': ['concept', 'match concept', 'ml concept'], 'dendrogram': ['dendrogram'], 'qualiti': ['qualiti'], 'specif': ['specif'], 'depth': ['depth', 'depth explan'], 'special': ['special', 'ml special', 'machin learn special'], 'encount': ['encount'], 'understand': ['understand'], 'valu': ['valu'], 'math': ['math', 'math background'], 'middl': ['middl'], 'learn': ['learn', 'machin learn', 'deep learn capston', 'learn task', 'machin learn techniqu', 'machin learn special'], 'background': ['background', 'math background'], 'tool': ['tool'], 'panda': ['panda'], 'dive': ['dive'], 'ml': ['ml', 'ml algorithm', 'ml concept', 'ml research', 'ml special'], 'web': ['web'], 'knn': ['knn'], 'assign': ['assign', 'program assign'], 'term': ['term'], 'good': ['good'], 'materi': ['materi', 'read materi'], 'way': ['way'], 'sens': ['sens'], 'accomplish': ['accomplish'], 'instruct': ['instruct'], 'time': ['time'], 'week': ['week'], 'view': ['view'], 'section': ['section'], 'method': ['method'], 'cliff': ['cliff'], 'feedback': ['feedback'], 'learner': ['learner'], 'pace': ['pace'], 'languag': ['languag'], 'code': ['code', 'code exampl'], 'carlo': ['carlo', 'carlo excel cours'], 'insight': ['insight'], 'forum': ['forum'], 'peer': ['peer'], 'capston': ['capston', 'capston project', 'deep learn capston'], 'job': ['job'], 'date': ['date'], 'mooc': ['mooc'], 'bit': ['bit'], 'theori': ['theori'], 'techniqu': ['techniqu', 'machin learn techniqu'], 'detail': ['detail'], 'idea': ['idea'], 'etho': ['etho'], 'aspect': ['aspect'], 'content': ['content'], 'algorithm': ['algorithm', 'algorithm applic', 'ml algorithm', 'em algorithm'], 'foundat': ['foundat', 'foundat cours'], 'read': ['read', 'read materi'], 'explan': ['explan', 'depth explan'], 'review': ['review'], 'use': ['use'], 'coverag': ['coverag'], 'approach': ['approach'], 'machin': ['machin', 'machin learn', 'hand-on lab machin', 'machin learn techniqu', 'machin learn special'], 'challeng': ['challeng'], 'list': ['list'], 'beginn': ['beginn'], 'grip': ['grip'], 'treatment': ['treatment'], 'subject': ['subject'], 'retriv': ['retriv'], 'someon': ['someon'], 'video': ['video'], 'survey': ['survey'], 'asses': ['asses'], 'point': ['point'], 'homework': ['homework'], 'experi': ['experi'], 'introduct': ['introduct'], 'practic': ['practic'], 'manner': ['manner'], 'awesom': ['awesom'], 'other': ['other'], 'hit': ['hit'], 'respect': ['respect'], 'compon': ['compon'], 'formula': ['formula'], 'univers': ['univers'], 'graphlab user': ['graphlab user'], 'recommend system': ['recommend system'], 'graph lab': ['graph lab'], 'kd tree': ['kd tree'], 'professor end': ['professor end'], 'dimension reduct': ['dimension reduct'], 'prof fox': ['prof fox'], 'world analyt': ['world analyt']}\n",
      "53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'algorithm': 6,\n",
       " 'assign': 6,\n",
       " 'background': 4,\n",
       " 'bit': 3,\n",
       " 'book': 2,\n",
       " 'capston': 6,\n",
       " 'carlo': 4,\n",
       " 'cluster': 10,\n",
       " 'code': 4,\n",
       " 'concept': 8,\n",
       " 'content': 4,\n",
       " 'cours': 71,\n",
       " 'depth': 3,\n",
       " 'detail': 3,\n",
       " 'emili': 6,\n",
       " 'explan': 4,\n",
       " 'feedback': 2,\n",
       " 'foundat': 3,\n",
       " 'idea': 2,\n",
       " 'insight': 3,\n",
       " 'instruct': 2,\n",
       " 'instructor': 6,\n",
       " 'intuit': 2,\n",
       " 'job': 2,\n",
       " 'knowledg': 4,\n",
       " 'lda': 4,\n",
       " 'learn': 12,\n",
       " 'learner': 2,\n",
       " 'lot': 9,\n",
       " 'machin': 9,\n",
       " 'materi': 13,\n",
       " 'math': 4,\n",
       " 'method': 2,\n",
       " 'ml': 7,\n",
       " 'pace': 2,\n",
       " 'part': 3,\n",
       " 'peopl': 3,\n",
       " 'qualiti': 2,\n",
       " 'read': 2,\n",
       " 'retriev': 4,\n",
       " 'special': 10,\n",
       " 'stuff': 2,\n",
       " 'subject': 2,\n",
       " 'teacher': 2,\n",
       " 'techniqu': 2,\n",
       " 'thank': 4,\n",
       " 'theori': 3,\n",
       " 'time': 4,\n",
       " 'tool': 2,\n",
       " 'topic': 9,\n",
       " 'understand': 5,\n",
       " 'way': 2,\n",
       " 'week': 3}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for key in G_dict.keys():\n",
    "    tk_key = nltk.word_tokenize(key)\n",
    "    if len(tk_key) == 1:\n",
    "        new_dict[tk_key[0]] = tk_key\n",
    "for key in G_dict.keys():\n",
    "    i = len(new_dict)\n",
    "    tk_key = nltk.word_tokenize(key)\n",
    "    flag = False\n",
    "    if len(tk_key) > 1:\n",
    "        for tk_k in tk_key:\n",
    "            if tk_k in new_dict.keys():\n",
    "                new_dict[tk_k].append(key) \n",
    "                flag =True\n",
    "        if flag == False:\n",
    "            new_dict[key] = [key]\n",
    "print(len(new_dict))\n",
    "print(len(G_dict))\n",
    "print(new_dict)\n",
    "count_dict = {}\n",
    "for key in new_dict.keys():\n",
    "    count_dict[key] = 0\n",
    "    for item in new_dict[key]:\n",
    "        count_dict[key] += G_dict[item]\n",
    "count_dict\n",
    "new_count_dict = {}\n",
    "for key in count_dict.keys():\n",
    "    if count_dict[key] >= 2:\n",
    "        new_count_dict[key] = count_dict[key]\n",
    "print(len(new_count_dict))\n",
    "new_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cours': 65, 'lot': 9, 'thank': 4, 'intuit': 2, 'instructor': 6, 'capston project': 3, 'stuff': 2, 'topic': 7, 'part': 3, 'lda': 4, 'cluster': 8, 'retriev': 4, 'emili': 4, 'knowledg': 4, 'peopl': 3, 'teacher': 2, 'book': 2, 'concept': 6, 'qualiti': 2, 'depth': 2, 'special': 8, 'machin learn': 5, 'understand': 5, 'classif cours': 2, 'math': 2, 'learn': 3, 'background': 2, 'tool': 2, 'ml': 3, 'assign': 5, 'materi': 12, 'way': 2, 'time': 4, 'math background': 2, 'week': 3, 'method': 2, 'feedback': 2, 'learner': 2, 'advanc topic': 2, 'pace': 2, 'code': 3, 'carlo': 3, 'insight': 3, 'capston': 2, 'job': 2, 'bit': 3, 'theori': 3, 'detail': 3, 'idea': 2, 'content': 4, 'algorithm': 3, 'foundat': 2, 'explan': 3, 'subject': 2}\n"
     ]
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for key in G_dict.keys():\n",
    "    if G_dict[key] >= 2:\n",
    "        new_dict[key] = G_dict[key]\n",
    "print(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = new_transaction['comment'][2]\n",
    "s =process_sentence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'cours': ['courses', 'course'], 'code exampl': ['code examples'], 'algorithm applic': ['algorithm applications'], 'intuit': ['intuition'], 'schedul': ['schedules'], 'instructor': ['instructors'], 'capston project': ['capstone project']}\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transaction['target'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'emili fox': ['Emily Fox'], 'job': ['job'], 'concept': ['concepts'], 'explan': ['explanation'], 'compon': ['components'], 'formula': ['formulas'], 'materi': ['materials'], 'cours': ['course'], 'univers': ['University'], 'machin learn special': ['machine learning specialization']}\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transaction['target'][99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"Emily Fox did a great job in explaining tough concepts with simple explanation of the components in the formulas It's a little tough to get through the materials though, it's the 4th course in University of Washington's machine learning specialization afterall =\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from nltk.parse.corenlp import CoreNLPServer, CoreNLPDependencyParser\\npath_to_jar = '/Users/collin/stanford/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar'\\npath_to_models_jar = '/Users/collin/stanford/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0-models.jar'\\nserver = CoreNLPServer(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\\nserver.start()\\ndependency_parser = CoreNLPDependencyParser()\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from nltk.parse.corenlp import CoreNLPServer, CoreNLPDependencyParser\n",
    "path_to_jar = '/Users/collin/stanford/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar'\n",
    "path_to_models_jar = '/Users/collin/stanford/stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0-models.jar'\n",
    "server = CoreNLPServer(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "server.start()\n",
    "dependency_parser = CoreNLPDependencyParser()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependency_parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ddb0e9dabb35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdep_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dependency_parser' is not defined"
     ]
    }
   ],
   "source": [
    "result = dependency_parser.raw_parse(s)\n",
    "dep = next(result)\n",
    "dep_list = dict(sorted(dep.nodes.items()))\n",
    "print(dep_list)\n",
    "for _, node in sorted(dep.nodes.items()):\n",
    "    print(node)\n",
    "    oo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_update(target_dict, new_w, word):\n",
    "    if new_w in target_dict.keys():\n",
    "        target_dict[new_w].append(word)\n",
    "    else:\n",
    "        target_dict[new_w] = [word]\n",
    "def extract_target_phase(target_dict, s):\n",
    "    ##parse the sentence\n",
    "    result = dependency_parser.raw_parse(s)\n",
    "    dep = next(result)\n",
    "    dep_list = dict(sorted(dep.nodes.items()))\n",
    "\n",
    "    for _, node in sorted(dep.nodes.items()):\n",
    "         if node['word'] is not None:\n",
    "            w = stem(node['word'])\n",
    "            dep_dict = dict(node['deps'])\n",
    "            if w in target_list and node['tag'][:2] == 'NN' and 'compound' in dep_dict.keys():\n",
    "                #print(w)\n",
    "                new_w =''\n",
    "                word = ''\n",
    "                for index in dep_dict['compound']:\n",
    "                    comp_word = dep_list[index]['word']\n",
    "                    comp_w = stem(comp_word)\n",
    "                    new_w = new_w + ' ' + comp_w\n",
    "                    word = word + ' ' + comp_word\n",
    "                new_w = new_w + ' ' + w\n",
    "                word = word + ' ' + node['word']\n",
    "                dict_update(target_dict, new_w, word)\n",
    "            elif w in target_list and node['tag'][:2] == 'NN' and node['rel'] !='compound':\n",
    "                dict_update(target_dict, w, node['word'])\n",
    "\n",
    "def update_target(sents):\n",
    "    target_d = {}\n",
    "    sent_list = sent_tokenize(sents)\n",
    "    for sent in sent_list:\n",
    "        extract_target_phase(target_d,sent)\n",
    "    for key in target_d.keys():\n",
    "        target_d[key] = list(set(target_d[key]))\n",
    "    return target_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependency_parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a048dff016e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtarget_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-451296aab057>\u001b[0m in \u001b[0;36mupdate_target\u001b[0;34m(sents)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0msent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mextract_target_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtarget_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-451296aab057>\u001b[0m in \u001b[0;36mextract_target_phase\u001b[0;34m(target_dict, s)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_target_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m##parse the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdep_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dependency_parser' is not defined"
     ]
    }
   ],
   "source": [
    "f1 = open('opinion_list_1.txt','r')\n",
    "lines = f1.readlines()\n",
    "opinion_list =[line.split(\"\\n\")[0] for line in lines]\n",
    "\n",
    "f2 = open('target_list_1.txt','r')\n",
    "lines = f2.readlines()\n",
    "target_list = [line.split(\"\\n\")[0] for line in lines]\n",
    "update_target(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "server.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
